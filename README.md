# ğŸ§  Cliente Ollama - Modelo phi3:mini

AplicaÃ§Ã£o simples em Python que se conecta Ã  API do Ollama e envia prompts ao modelo `phi3:mini`. O usuÃ¡rio digita uma mensagem, a aplicaÃ§Ã£o envia para a IA e exibe a resposta no terminal.

**Este projeto foi desenvolvido por Hailton Neto e Vinicius Macedo.**

---

## ğŸ“¦ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/hailtonneto/ollama-client.git
   cd ollama-client
   ```
2. **Crie um ambiente virtual (opcional, mas recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    venv\Scripts\activate     # Windows
    ```
3. **Instale as dependÃªncias:**
    - Forma direta:
        ```bash
        pip install requests
        ```
    - Ou usando o arquivo requirements.txt:
        ```bash
        pip install -r requirements.txt
        ```

---

## ğŸš€ Como usar

1. **Execute o script:**
    ```bash
    python main.py
    ```
2. **Digite seu prompt no terminal e veja a resposta da IA.**
3. **Para sair, digite `sair`.**

---

## ğŸ”§ ConfiguraÃ§Ãµes

- URL da API: http://ollama.eastus.cloudapp.azure.com:11434/
- Modelo usado: phi3:mini
- Formato da requisiÃ§Ã£o:
    ```bash
    {
    "model": "phi3:mini",
    "prompt": "sua mensagem aqui",
    "stream": false
    }
    ```

---

## ğŸ“ Estrutura do Projeto

```bash
ollama-client/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“„ Exemplo de uso

```bash
ğŸ§  Cliente Ollama - Modelo phi3:mini

Digite seu prompt (ou 'sair' para encerrar): Qual a capital da FranÃ§a?

ğŸ’¬ Resposta da IA:
A capital da FranÃ§a Ã© Paris.
```

---

